from pyspark.sql.functions import col, lit, regexp_replace
from pyspark.sql.types import StringType

from lib.pt_utils import *
from lib.IncrementalLoader import IncrementalLoader
from lib.PineconeManager import PineconeManager, prepare_data
from pyspark.sql.functions import col, lit, max as spark_max, length, row_number, explode, unix_timestamp, from_unixtime, date_format, to_timestamp, concat, expr

is_gcs_enabled = os.getenv('IS_GCS_ENABLED')
pinecone_key = os.getenv('PINECONE_KEY')
index_name = "letstalkvector"
namespace = "letstalk-ns"

pi = PineconeManager(index_name, namespace, pinecone_key)

if is_gcs_enabled.lower() == 'true':
    is_gcs_enabled = True
else:
    is_gcs_enabled = False

spark, base_path = get_spark_and_path(is_gcs_enabled)
sc = spark.sparkContext
sc.addPyFile("/script/lib.zip")

trusted_path = get_trusted_path(base_path)


table_subpath = f'delta_sports/matches'
loader = IncrementalLoader(spark, trusted_path, table_subpath, is_gcs_enabled)
matches = loader.get_new_data()

if matches.isEmpty():
    logging.info(f"No new data for {table_subpath}")
else:
    sports_path = os.path.join(trusted_path, 'delta_sports')
    league = spark.read.format("delta").load(os.path.join(sports_path, "leagues")).select("league_id", "league_name")
    teams = spark.read.format("delta").load(os.path.join(sports_path, "teams")).select("team_id", "team_name")
    venues = spark.read.format("delta").load(os.path.join(sports_path, "venues")).select("venue_id", "venue_name")

    home_teams = teams.alias("home_teams")
    away_teams = teams.alias("away_teams")

    enriched_matches = (matches
                   .join(league, on=(col("league_id") == col("league")))
                   .join(home_teams, on=(col("home_teams.team_id") == col("team_home_id")))
                   .withColumnRenamed("team_name", "team_home")
                   .join(away_teams, on=(col("away_teams.team_id") == col("team_away_id")))
                   .withColumnRenamed("team_name", "team_away")
                   .join(venues, on=(matches.venue_id == venues.venue_id))
                   .drop("league", "league_id", "team_id", "team_away_id", "team_home_id",
                         "venue_id", "period_first", "period_second", "referee", "status_elapsed", "status_extra")
                   .withColumn(
        "match_date",
        date_format(to_timestamp(col("fixture_date")), "dd MMMM yyyy")
    )
                   .withColumn(
        "text_to_embed",
        concat(col("league_name"), lit(" "), col("team_home"), lit(" - "), col("team_away"), lit(" "), col("match_date"))
    )
                   .filter(col("status_long") == "match finished")
                   .drop("status_long", "fixture_date")
                   .withColumn("fixture_id", col("fixture_id").cast(StringType()))
                   )

    pc_data, pc_reg = prepare_data(enriched_matches, "fixture_id", "timestamp", ["text_to_embed"])

    pinecone_loader = pi.get_pinecone_loader()
    pc_data.rdd.foreachPartition(pinecone_loader)

    loader.update_control_table()
    logging.info("Data was uploaded to pinecone")

spark.stop()


from pyspark.sql.functions import col, lit, regexp_replace

from lib.pt_utils import *
from lib.IncrementalLoader import IncrementalLoader
from lib.PineconeManager import PineconeManager, prepare_data

is_gcs_enabled = os.getenv('IS_GCS_ENABLED')
pinecone_key = os.getenv('PINECONE_KEY')
index_name = "letstalkvector"
namespace = "letstalk-ns"

pi = PineconeManager(index_name, namespace, pinecone_key)

if is_gcs_enabled.lower() == 'true':
    is_gcs_enabled = True
else:
    is_gcs_enabled = False

spark, base_path = get_spark_and_path(is_gcs_enabled)
sc = spark.sparkContext
sc.addPyFile("/script/lib.zip")

trusted_path = get_trusted_path(base_path)


CATEGORIES = ['entertainment', 'sports', 'technology']
for category in CATEGORIES:
    logging.info(f"Processing category {category}")
    table_subpath = f'delta_news/{category}'
    loader = IncrementalLoader(spark, trusted_path, table_subpath, is_gcs_enabled)
    df = loader.get_new_data()

    if df.isEmpty():
        logging.info(f"No new data for {category}")
        continue

    df = df.withColumn(
        "url",
        regexp_replace(col("url"), r"[^\x00-\x7F]", ""))


    pc_data, pc_registry = prepare_data(df, key_col="url", date_col="publishedAt", text_to_embed_cols=["content", "title", "description"], source=table_subpath)

    pinecone_loader = pi.get_pinecone_loader()
    pc_data.rdd.foreachPartition(pinecone_loader)

    loader.update_control_table()


spark.stop()
logging.info("Data was uploaded to pinecone")


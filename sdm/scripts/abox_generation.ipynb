{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5d82a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import names\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "sys.path.append(os.path.abspath(\"../../\")) \n",
    "\n",
    "# Standard libraries\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "from rdflib import Graph, Namespace, URIRef, Literal\n",
    "from rdflib.namespace import RDF, RDFS, XSD\n",
    "\n",
    "# Your project modules\n",
    "from dags.lib.pt_utils import *\n",
    "from dags.lib.IncrementalLoader import IncrementalLoader\n",
    "from dags.lib.Processer import *\n",
    "\n",
    "# Spark\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from delta import *\n",
    "\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f339eafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "DBO = Namespace(\"http://sdm_upc.org/ontology/\")\n",
    "DBR = Namespace(\"http://sdm_upc.org/resource/\")\n",
    "\n",
    "\n",
    "g.bind(\"dbo\", DBO)\n",
    "g.bind(\"dbr\", DBR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "820a44bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consistent_hash(value):\n",
    "    return int(hashlib.sha256(str(value).encode()).hexdigest(), 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3ffd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "USERS=100\n",
    "PERCENTAGE=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e9bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_gcs_enabled= \"False\"\n",
    "if is_gcs_enabled.lower() == 'true':\n",
    "    is_gcs_enabled = True\n",
    "else:\n",
    "    is_gcs_enabled = False\n",
    "\n",
    "spark, base_path = get_spark_and_path(is_gcs_enabled)\n",
    "\n",
    "trusted_path ='..\\..\\data\\letstalk_trusted_zone_bdma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fc8ea51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:dags.lib.pt_utils:False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:dags.lib.pt_utils:False\n",
      "INFO:py4j.clientserver:Error while receiving.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\josub\\.conda\\envs\\spark311\\Lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=4736>\n",
      "INFO:py4j.clientserver:Closing down clientserver connection\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\josub\\.conda\\envs\\spark311\\Lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=4736>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\josub\\.conda\\envs\\spark311\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\josub\\.conda\\envs\\spark311\\Lib\\site-packages\\py4j\\clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "INFO:py4j.clientserver:Error while receiving.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\josub\\.conda\\envs\\spark311\\Lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\josub\\.conda\\envs\\spark311\\Lib\\socket.py\", line 718, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\josub\\.conda\\envs\\spark311\\Lib\\site-packages\\pyspark\\context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"c:\\Users\\josub\\.conda\\envs\\spark311\\Lib\\site-packages\\pyspark\\context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "    ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\josub\\.conda\\envs\\spark311\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\josub\\.conda\\envs\\spark311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\josub\\.conda\\envs\\spark311\\Lib\\site-packages\\py4j\\protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o22.sc\n",
      "INFO:py4j.clientserver:Closing down clientserver connection\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\josub\\.conda\\envs\\spark311\\Lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\josub\\.conda\\envs\\spark311\\Lib\\socket.py\", line 718, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\josub\\.conda\\envs\\spark311\\Lib\\site-packages\\pyspark\\context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"c:\\Users\\josub\\.conda\\envs\\spark311\\Lib\\site-packages\\pyspark\\context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "    ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\josub\\.conda\\envs\\spark311\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\josub\\.conda\\envs\\spark311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\josub\\.conda\\envs\\spark311\\Lib\\site-packages\\py4j\\protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o22.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\josub\\.conda\\envs\\spark311\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\josub\\.conda\\envs\\spark311\\Lib\\site-packages\\py4j\\clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "INFO:py4j.clientserver:Closing down clientserver connection\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling z:io.delta.tables.DeltaTable.forPath",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m subpath= \u001b[33m'\u001b[39m\u001b[33mmovie\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     36\u001b[39m path = os.path.join(trusted_path, subpath)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m df = \u001b[43mDeltaTable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m.toDF().sample(fraction=PERCENTAGE, seed=\u001b[32m42\u001b[39m)\n\u001b[32m     38\u001b[39m film_ids = df.select(\u001b[33m\"\u001b[39m\u001b[33mfilm_id\u001b[39m\u001b[33m\"\u001b[39m).distinct().rdd.map(\u001b[38;5;28;01mlambda\u001b[39;00m row: row.film_id).collect()\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m df.toLocalIterator():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josub\\.conda\\envs\\spark311\\Lib\\site-packages\\delta\\tables.py:387\u001b[39m, in \u001b[36mDeltaTable.forPath\u001b[39m\u001b[34m(cls, sparkSession, path, hadoopConf)\u001b[39m\n\u001b[32m    384\u001b[39m jvm: \u001b[33m\"\u001b[39m\u001b[33mJVMView\u001b[39m\u001b[33m\"\u001b[39m = sparkSession._sc._jvm  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    385\u001b[39m jsparkSession: \u001b[33m\"\u001b[39m\u001b[33mJavaObject\u001b[39m\u001b[33m\"\u001b[39m = sparkSession._jsparkSession  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m jdt = \u001b[43mjvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtables\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDeltaTable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjsparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhadoopConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DeltaTable(sparkSession, jdt)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josub\\.conda\\envs\\spark311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josub\\.conda\\envs\\spark311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josub\\.conda\\envs\\spark311\\Lib\\site-packages\\py4j\\protocol.py:334\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    330\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    335\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    336\u001b[39m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name))\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28mtype\u001b[39m = answer[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mPy4JError\u001b[39m: An error occurred while calling z:io.delta.tables.DeltaTable.forPath"
     ]
    }
   ],
   "source": [
    "def keyword_extract(row):\n",
    "    texts = []\n",
    "\n",
    "    if row.content is not None:\n",
    "        texts.append(row.content)\n",
    "\n",
    "    if row.title is not None:\n",
    "        texts.append(row.title)\n",
    "\n",
    "    if row.description is not None:\n",
    "        texts.append(row.description)\n",
    "        \n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "\n",
    "        # Sum TF-IDF scores across all documents\n",
    "    scores = X.sum(axis=0).A1\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    word_scores = list(zip(words, scores))\n",
    "\n",
    "    sorted_words = sorted(word_scores, key=lambda x: x[1], reverse=True)\n",
    "    top_words = [word for word, _ in sorted_words[:3]]\n",
    "    return top_words\n",
    "    \n",
    "is_gcs_enabled= \"False\"\n",
    "if is_gcs_enabled.lower() == 'true':\n",
    "    is_gcs_enabled = True\n",
    "else:\n",
    "    is_gcs_enabled = False\n",
    "\n",
    "spark, base_path = get_spark_and_path(is_gcs_enabled)\n",
    "\n",
    "trusted_path ='..\\..\\data\\letstalk_trusted_zone_bdma'\n",
    "subpath= 'movie'\n",
    "\n",
    "path = os.path.join(trusted_path, subpath)\n",
    "df = DeltaTable.forPath(spark, path).toDF().sample(fraction=PERCENTAGE, seed=42)\n",
    "film_ids = df.select(\"film_id\").distinct().rdd.map(lambda row: row.film_id).collect()\n",
    "for row in df.toLocalIterator():\n",
    "\n",
    "    subject = URIRef(DBR+f\"film_{row.film_id}\")\n",
    "    \n",
    "    g.add((subject, DBO.movie_title, Literal(str(row.title), datatype=XSD.string )))\n",
    "    g.add((subject, DBO.movie_language, Literal(str(row.original_title), datatype=XSD.string )))\n",
    "    g.add((subject, DBO.movie_release_date, Literal(row.release_date, datatype=XSD.date )))\n",
    "    g.add((subject, DBO.movie_revenue, Literal(int(row.revenue), datatype=XSD.integer)))\n",
    "    g.add((subject, DBO.movie_budget, Literal(int(row.budget), datatype=XSD.integer )))\n",
    "    runtime_value = row.runtime\n",
    "    if runtime_value is not None and not math.isnan(runtime_value):\n",
    "        g.add((subject, DBO.movie_runtime, Literal(int(runtime_value), datatype=XSD.integer)))\n",
    "    g.add((subject, DBO.movie_adult, Literal(bool(row.adult), datatype=XSD.bolean)))\n",
    "    g.add((subject, DBO.movie_popularity, Literal(float(row.popularity), datatype=XSD.long)))\n",
    "    g.add((subject, DBO.movie_vote_avg, Literal(float(row.vote_average), datatype=XSD.long)))\n",
    "    g.add((subject, DBO.movie_vote_cnt, Literal(int(row.vote_count), datatype=XSD.integer)))\n",
    "\n",
    "subpath= 'movie_genre'\n",
    "\n",
    "path = os.path.join(trusted_path, subpath)\n",
    "df = DeltaTable.forPath(spark, path).toDF().sample(fraction=PERCENTAGE, seed=42)\n",
    "\n",
    "for row in df.toLocalIterator():\n",
    "    subject = URIRef(DBR+f\"film_{row.film_id}\")\n",
    "    object = URIRef(DBR+f\"genre_{row.genre_id}\")\n",
    "    g.add((subject, DBO.has_genre, object))\n",
    "    \n",
    "subpath= 'genre'\n",
    "\n",
    "path = os.path.join(trusted_path, subpath)\n",
    "df = DeltaTable.forPath(spark, path).toDF().sample(fraction=PERCENTAGE, seed=42)\n",
    "for row in df.toLocalIterator():\n",
    "    subject = URIRef(DBR+f\"genre_{row.genre_id}\")\n",
    "    g.add((subject, DBO.genre_name, Literal(str(row.genre), datatype=XSD.string )))\n",
    "    \n",
    "\n",
    "subpath= 'entertainment'\n",
    "\n",
    "path = os.path.join(trusted_path, subpath)\n",
    "df = DeltaTable.forPath(spark, path).toDF().sample(fraction=0.1, seed=42)\n",
    "for row in df.toLocalIterator():\n",
    "    subject = URIRef(DBR+f\"new_{consistent_hash(row.url)}\")\n",
    "    g.add((subject, RDFS.type, DBO.Entertainment_News))\n",
    "    g.add((subject, DBO.written_by, URIRef(DBR+f\"journalist_{consistent_hash(row.author)}\")))\n",
    "    g.add((URIRef(DBR+f\"journalist_{consistent_hash(row.author)}\"), DBO.author_name, Literal(str(row.author), datatype=XSD.string )))\n",
    "    for keyword in keyword_extract(row):\n",
    "        g.add((subject, DBO.related_keyword, URIRef(DBR+f\"keyword_{consistent_hash(keyword)}\")))\n",
    "        g.add(( URIRef(DBR+f\"keyword_{consistent_hash(keyword)}\"), DBO.keyword_text, Literal(str(keyword), datatype=XSD.string )))\n",
    "    \n",
    "    g.add((subject, DBO.published_at,  URIRef(DBR+f\"source_{consistent_hash(row.source)}\")))\n",
    "    g.add((URIRef(DBR+f\"source_{consistent_hash(row.source)}\"), DBO.source_name, Literal(str(row.source), datatype=XSD.string )))\n",
    "    g.add((subject, DBO.news_title, Literal(str(row.title), datatype=XSD.string )))\n",
    "    g.add((subject, DBO.news_date, Literal(row.release_date, datatype=XSD.dateTime)))\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "for i in range(USERS):\n",
    "    subject = URIRef(DBR+f\"user_{i}\")\n",
    "    g.add((subject, DBO.user_name, Literal(str(names.get_full_name()), datatype=XSD.string )))\n",
    "    for j in range(random.randint(0, 3)):\n",
    "        g.add((subject, DBO.likes_movie, URIRef(DBR+f\"film_{random.choice(film_ids)}\")))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72d6fd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:dags.lib.pt_utils:False\n",
      "INFO:dags.lib.pt_utils:False\n"
     ]
    }
   ],
   "source": [
    "is_gcs_enabled= \"False\"\n",
    "if is_gcs_enabled.lower() == 'true':\n",
    "    is_gcs_enabled = True\n",
    "else:\n",
    "    is_gcs_enabled = False\n",
    "\n",
    "spark, base_path = get_spark_and_path(is_gcs_enabled)\n",
    "\n",
    "trusted_path ='..\\..\\data\\letstalk_trusted_zone_bdma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e0ce44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat_ws, when\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "def extract_column_keywords(df, text_columns=['content', 'title', 'description'], top_n=10):\n",
    "    \"\"\"\n",
    "    Returns: List of top keywords (without frequencies)\n",
    "    \"\"\"\n",
    "    combined_df = df.withColumn(\n",
    "        \"combined_text\",\n",
    "        concat_ws(\" \", *[\n",
    "            when(col(c).isNotNull(), col(c)).otherwise(\"\")\n",
    "            for c in text_columns\n",
    "        ]))\n",
    "    \n",
    "    texts = [row.combined_text for row in combined_df.select(\"combined_text\").collect()]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    keywords = []\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        scores = X[i].toarray().flatten()\n",
    "        top_indices = scores.argsort()[-3:][::-1]\n",
    "        keywords.extend(feature_names[top_indices])\n",
    "    \n",
    "    # Return just the words as a list\n",
    "    return [word for word, count in Counter(keywords).most_common(top_n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82a5e93a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m subpath= \u001b[33m'\u001b[39m\u001b[33mtechnology\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      5\u001b[39m path = os.path.join(trusted_path, subpath)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df = DeltaTable.forPath(\u001b[43mspark\u001b[49m, path).toDF()\n\u001b[32m      8\u001b[39m top_keywords = extract_column_keywords(df, top_n=\u001b[32m25\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m( top_keywords)\n",
      "\u001b[31mNameError\u001b[39m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "trusted_path ='..\\..\\data\\letstalk_trusted_zone_bdma'\n",
    "    \n",
    "subpath= 'technology'\n",
    "\n",
    "path = os.path.join(trusted_path, subpath)\n",
    "df = DeltaTable.forPath(spark, path).toDF()\n",
    "\n",
    "top_keywords = extract_column_keywords(df, top_n=25)\n",
    "print( top_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7079d003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, RegexTokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, concat_ws, when, posexplode, lit, array\n",
    "import os\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Load data\n",
    "path = os.path.join(trusted_path, \"movie\")\n",
    "movies_df = DeltaTable.forPath(spark, path).toDF()\n",
    "path = os.path.join(trusted_path, \"entertainment\")\n",
    "news_df = DeltaTable.forPath(spark, path).toDF()\n",
    "\n",
    "def find_movie_mentions(news_df, movies_df, text_columns=['content', 'title', 'description']):\n",
    "    # Step 1: Prepare movie vocabulary (lowercase)\n",
    "    movie_vocab = [row.title.lower() for row in movies_df.select(\"title\").distinct().collect()]\n",
    "    \n",
    "    # Step 2: Combine text columns in news data\n",
    "    news_combined = news_df.withColumn(\n",
    "        \"combined_text\",\n",
    "        concat_ws(\" \", *[\n",
    "            when(col(c).isNotNull(), col(c)).otherwise(\"\")\n",
    "            for c in text_columns\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    # Step 3: Tokenize text\n",
    "    tokenizer = RegexTokenizer(\n",
    "        inputCol=\"combined_text\",\n",
    "        outputCol=\"words\",\n",
    "        pattern=r\"\\W+\",\n",
    "        toLowercase=True\n",
    "    )\n",
    "    \n",
    "    # Step 4: Configure CountVectorizer\n",
    "    cv = CountVectorizer(\n",
    "        inputCol=\"words\",\n",
    "        outputCol=\"movie_hits\",\n",
    "        vocabSize=100000,  # Large enough to capture all terms\n",
    "        minDF=1.0\n",
    "    )\n",
    "    \n",
    "    # Step 5: Build and run pipeline\n",
    "    pipeline = Pipeline(stages=[tokenizer, cv])\n",
    "    model = pipeline.fit(news_combined)\n",
    "    \n",
    "    # Step 6: Get vocabulary and find movie indices\n",
    "    vocab = model.stages[-1].vocabulary\n",
    "    movie_indices = [i for i, word in enumerate(vocab) if word in movie_vocab]\n",
    "    \n",
    "    # Step 7: Transform data and find matches\n",
    "    result = model.transform(news_combined).select(\n",
    "        \"title\",\n",
    "        posexplode(\"movie_hits\").alias(\"pos\", \"count\")\n",
    "    ).filter(\n",
    "        (col(\"pos\").isin(movie_indices)) & \n",
    "        (col(\"count\") > 0)\n",
    "    ).withColumn(\n",
    "        \"movie_title\",\n",
    "        array([lit(vocab[i]) for i in movie_indices])[col(\"pos\")]\n",
    "    )\n",
    "    \n",
    "    return result.select(\"title\", \"movie_title\", \"count\")\n",
    "\n",
    "# Execute and show results\n",
    "matches_df = find_movie_mentions(news_df, movies_df)\n",
    "matches_df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-29T17:11:44.787987Z",
     "start_time": "2025-04-29T17:11:44.786297Z"
    }
   },
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "import os\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from pyspark.sql import Row"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T17:11:46.452873Z",
     "start_time": "2025-04-29T17:11:46.450401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_spark_session():\n",
    "    conf = (\n",
    "        pyspark.conf.SparkConf()\n",
    "        .setAppName(\"LetsTalk\")\n",
    "        .set(\n",
    "            \"spark.sql.catalog.spark_catalog\",\n",
    "            \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "        )\n",
    "        .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .set(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "        .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\")\n",
    "        .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/Users/alfio/projects/upc/BDMP2/gcs.json\")\n",
    "        .set(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "        .set(\"spark.jars\", \"../jars/gcs-connector-hadoop3-latest.jar\")\n",
    "        .setMaster(\n",
    "            \"local[*]\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    builder = pyspark.sql.SparkSession.builder.appName(\"LetsTalk\").config(conf=conf)\n",
    "    spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "    return spark\n",
    "\n"
   ],
   "id": "a0fed999dd78466a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T17:11:51.592381Z",
     "start_time": "2025-04-29T17:11:49.476248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = create_spark_session()\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ],
   "id": "314d425a93c6dfe7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/29 19:11:50 WARN Utils: Your hostname, Alfios-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.50.4 instead (on interface en0)\n",
      "25/04/29 19:11:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/alfio/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/alfio/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-27f92732-6c93-4835-8500-9acecc8b042c;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/alfio/python_venv/general/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound io.delta#delta-spark_2.12;3.3.0 in central\n",
      "\tfound io.delta#delta-storage;3.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 85ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-27f92732-6c93-4835-8500-9acecc8b042c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/2ms)\n",
      "25/04/29 19:11:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T17:12:07.964927Z",
     "start_time": "2025-04-29T17:12:07.410461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "landing_path = \"../data/letstalk_landing_zone_bdma/delta_news/entertainment\"\n",
    "deltane = DeltaTable.forPath(spark, landing_path)\n",
    "dfne = deltane.toDF()"
   ],
   "id": "16a0124018012ce5",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T13:15:33.106137Z",
     "start_time": "2025-04-29T13:15:33.101997Z"
    }
   },
   "cell_type": "code",
   "source": "new_data_df = dfne.filter(dfne.author.startswith(\"Ann\"))",
   "id": "d66c60a7e644abdd",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T13:15:35.879769Z",
     "start_time": "2025-04-29T13:15:35.334694Z"
    }
   },
   "cell_type": "code",
   "source": "new_data_df.write.format(\"delta\").mode(\"append\").save(landing_path)",
   "id": "3096743401fea141",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:55:00.539695Z",
     "start_time": "2025-04-29T12:55:00.530907Z"
    }
   },
   "cell_type": "code",
   "source": "deltane.filter(deltane.filter(''))",
   "id": "aa75e455a61814ae",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DeltaTable' object has no attribute 'insert'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[70], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdeltane\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minsert\u001B[49m()\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'DeltaTable' object has no attribute 'insert'"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T15:47:11.592401Z",
     "start_time": "2025-04-29T15:47:11.503658Z"
    }
   },
   "cell_type": "code",
   "source": "deltane.history()",
   "id": "cbc54909bce37c69",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/29 17:47:11 WARN DeltaHistoryManager: Found Delta commit 0 with a timestamp 1745921639145 which is greater than the next commit timestamp 1745921639145.\n",
      "25/04/29 17:47:11 WARN DeltaHistoryManager: Found Delta commit 2 with a timestamp 1745921639501 which is greater than the next commit timestamp 1745921639239.\n",
      "25/04/29 17:47:11 WARN DeltaHistoryManager: Found Delta commit 3 with a timestamp 1745921639502 which is greater than the next commit timestamp 1745921639185.\n",
      "25/04/29 17:47:11 WARN DeltaHistoryManager: Found Delta commit 5 with a timestamp 1745921639603 which is greater than the next commit timestamp 1745921639436.\n",
      "25/04/29 17:47:11 WARN DeltaHistoryManager: Found Delta commit 6 with a timestamp 1745921639604 which is greater than the next commit timestamp 1745921639330.\n",
      "25/04/29 17:47:11 WARN DeltaHistoryManager: Found Delta commit 7 with a timestamp 1745921639605 which is greater than the next commit timestamp 1745921639185.\n",
      "25/04/29 17:47:11 WARN DeltaHistoryManager: Found Delta commit 8 with a timestamp 1745921639606 which is greater than the next commit timestamp 1745921639294.\n",
      "25/04/29 17:47:11 WARN DeltaHistoryManager: Found Delta commit 9 with a timestamp 1745921639607 which is greater than the next commit timestamp 1745921639089.\n",
      "25/04/29 17:47:11 WARN DeltaHistoryManager: Found Delta commit 10 with a timestamp 1745921639608 which is greater than the next commit timestamp 1745921639185.\n",
      "25/04/29 17:47:11 WARN DeltaHistoryManager: Found Delta commit 11 with a timestamp 1745921639609 which is greater than the next commit timestamp 1745921639087.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "+-------+--------------------+------+--------+-----------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+--------------------+--------------------+\n",
       "|version|           timestamp|userId|userName|        operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|        userMetadata|          engineInfo|\n",
       "+-------+--------------------+------+--------+-----------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+--------------------+--------------------+\n",
       "|     14|2025-04-29 15:15:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|         13|  Serializable|        false|{numFiles -> 3, n...|                NULL|Apache-Spark/3.5....|\n",
       "|     13|2025-04-29 15:13:...|  NULL|    NULL|SET TBLPROPERTIES|{properties -> {\"...|NULL|    NULL|     NULL|         12|  Serializable|         true|                  {}|                NULL|Apache-Spark/3.5....|\n",
       "|     12|2025-04-29 12:13:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|         11|  Serializable|         true|{numFiles -> 1, n...|{\"inserted_rows\":...|Apache-Spark/3.5....|\n",
       "|     11|2025-04-29 12:13:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|         10|  Serializable|         true|{numFiles -> 1, n...|{\"inserted_rows\":...|Apache-Spark/3.5....|\n",
       "|     10|2025-04-29 12:13:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          9|  Serializable|         true|{numFiles -> 1, n...|{\"inserted_rows\":...|Apache-Spark/3.5....|\n",
       "|      9|2025-04-29 12:13:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          8|  Serializable|         true|{numFiles -> 1, n...|{\"inserted_rows\":...|Apache-Spark/3.5....|\n",
       "|      8|2025-04-29 12:13:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          7|  Serializable|         true|{numFiles -> 1, n...|{\"inserted_rows\":...|Apache-Spark/3.5....|\n",
       "|      7|2025-04-29 12:13:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          6|  Serializable|         true|{numFiles -> 1, n...|{\"inserted_rows\":...|Apache-Spark/3.5....|\n",
       "|      6|2025-04-29 12:13:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          4|  Serializable|         true|{numFiles -> 1, n...|{\"inserted_rows\":...|Apache-Spark/3.5....|\n",
       "|      5|2025-04-29 12:13:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          4|  Serializable|         true|{numFiles -> 1, n...|{\"inserted_rows\":...|Apache-Spark/3.5....|\n",
       "|      4|2025-04-29 12:13:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          3|  Serializable|         true|{numFiles -> 1, n...|{\"inserted_rows\":...|Apache-Spark/3.5....|\n",
       "|      3|2025-04-29 12:13:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          2|  Serializable|         true|{numFiles -> 1, n...|{\"inserted_rows\":...|Apache-Spark/3.5....|\n",
       "|      2|2025-04-29 12:13:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          1|  Serializable|         true|{numFiles -> 1, n...|{\"inserted_rows\":...|Apache-Spark/3.5....|\n",
       "|      1|2025-04-29 12:13:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          0|  Serializable|         true|{numFiles -> 1, n...|{\"inserted_rows\":...|Apache-Spark/3.5....|\n",
       "|      0|2025-04-29 12:13:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|       NULL|  Serializable|         true|{numFiles -> 1, n...|{\"inserted_rows\":...|Apache-Spark/3.5....|\n",
       "+-------+--------------------+------+--------+-----------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+--------------------+--------------------+"
      ],
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr>\n",
       "<tr><td>14</td><td>2025-04-29 15:15:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>13</td><td>Serializable</td><td>false</td><td>{numFiles -&gt; 3, n...</td><td>NULL</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>13</td><td>2025-04-29 15:13:...</td><td>NULL</td><td>NULL</td><td>SET TBLPROPERTIES</td><td>{properties -&gt; {&quot;...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>12</td><td>Serializable</td><td>true</td><td>{}</td><td>NULL</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>12</td><td>2025-04-29 12:13:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>11</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;inserted_rows&quot;:...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>11</td><td>2025-04-29 12:13:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>10</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;inserted_rows&quot;:...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>10</td><td>2025-04-29 12:13:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>9</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;inserted_rows&quot;:...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>9</td><td>2025-04-29 12:13:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>8</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;inserted_rows&quot;:...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>8</td><td>2025-04-29 12:13:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>7</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;inserted_rows&quot;:...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>7</td><td>2025-04-29 12:13:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>6</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;inserted_rows&quot;:...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>6</td><td>2025-04-29 12:13:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>4</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;inserted_rows&quot;:...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>5</td><td>2025-04-29 12:13:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>4</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;inserted_rows&quot;:...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>4</td><td>2025-04-29 12:13:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>3</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;inserted_rows&quot;:...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>3</td><td>2025-04-29 12:13:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>2</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;inserted_rows&quot;:...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>2</td><td>2025-04-29 12:13:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>1</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;inserted_rows&quot;:...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>1</td><td>2025-04-29 12:13:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>0</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;inserted_rows&quot;:...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>0</td><td>2025-04-29 12:13:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;inserted_rows&quot;:...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "</table>\n"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T13:19:01.950683Z",
     "start_time": "2025-04-29T13:19:01.936892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_data = spark.read.format(\"delta\") \\\n",
    "    .option(\"readChangeData\", \"true\") \\\n",
    "    .option(\"startingVersion\", 0) \\\n",
    "    .option(\"endingVersion\", 14) \\\n",
    "    .load(path=landing_path) \\\n",
    "    .filter(\"_change_type = 'insert'\")"
   ],
   "id": "d3fc49d93f1931f",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:53:17.306593Z",
     "start_time": "2025-04-29T12:53:17.302854Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 66,
   "source": "dfne = dfne.dropDuplicates()",
   "id": "47af411f91a66b59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:53:18.955085Z",
     "start_time": "2025-04-29T12:53:18.945071Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 67,
   "source": [
    "deltatu = DeltaTable.forPath(spark, \"../data/letstalk_landing_zone_bdma/delta_tmdb/upcoming\")\n",
    "dftu = deltatu.toDF()"
   ],
   "id": "b4beb5a6bc965c9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:49:17.234636Z",
     "start_time": "2025-04-29T12:49:17.071685Z"
    }
   },
   "cell_type": "code",
   "source": "deltatu.history()",
   "id": "d2e2e24eb17d09c5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+--------------------+--------------------+\n",
       "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|        userMetadata|          engineInfo|\n",
       "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+--------------------+--------------------+\n",
       "|      2|2025-04-29 12:14:...|  NULL|    NULL|    WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          1|  Serializable|         true|{numFiles -> 1, n...|{\"perc_rows_inser...|Apache-Spark/3.5....|\n",
       "|      1|2025-04-29 12:14:...|  NULL|    NULL|    WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          0|  Serializable|         true|{numFiles -> 1, n...|{\"perc_rows_inser...|Apache-Spark/3.5....|\n",
       "|      0|2025-04-29 12:14:...|  NULL|    NULL|    WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|       NULL|  Serializable|         true|{numFiles -> 1, n...|{\"perc_rows_inser...|Apache-Spark/3.5....|\n",
       "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+--------------------+--------------------+"
      ],
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr>\n",
       "<tr><td>2</td><td>2025-04-29 12:14:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>1</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;perc_rows_inser...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>1</td><td>2025-04-29 12:14:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>0</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;perc_rows_inser...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>0</td><td>2025-04-29 12:14:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;perc_rows_inser...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "</table>\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T11:47:55.775637Z",
     "start_time": "2025-04-29T11:47:55.672917Z"
    }
   },
   "cell_type": "code",
   "source": "dftu.groupby('id').count().orderBy('count', ascending=False).show()",
   "id": "fc2ab4db476d655f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|     id|count|\n",
      "+-------+-----+\n",
      "|1244944|    3|\n",
      "|1126166|    3|\n",
      "|1064486|    3|\n",
      "| 995926|    3|\n",
      "|1233575|    3|\n",
      "|1212855|    3|\n",
      "|1226406|    3|\n",
      "|1241436|    3|\n",
      "| 324544|    3|\n",
      "|1353117|    3|\n",
      "|1388366|    3|\n",
      "|1380415|    2|\n",
      "| 970450|    2|\n",
      "| 575265|    2|\n",
      "| 986056|    2|\n",
      "|1181107|    2|\n",
      "|1233413|    2|\n",
      "|1232546|    2|\n",
      "|1249289|    1|\n",
      "|1249213|    1|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T11:49:49.053619Z",
     "start_time": "2025-04-29T11:49:48.853633Z"
    }
   },
   "cell_type": "code",
   "source": "dftu.filter(dftu.id == 1244944)",
   "id": "8850a9322ff614c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-----+--------------------+----------+-------+-----------------+--------------------+--------------------+----------+--------------------+------------+--------------------+-----+------------+----------+--------------------+----------+----------+\n",
       "|adult|       backdrop_path| genre_ids|     id|original_language|      original_title|            overview|popularity|         poster_path|release_date|               title|video|vote_average|vote_count|      ingestion_time|begin_date|  end_date|\n",
       "+-----+--------------------+----------+-------+-----------------+--------------------+--------------------+----------+--------------------+------------+--------------------+-----+------------+----------+--------------------+----------+----------+\n",
       "|false|/3lEV4CoKoeT2cZ4f...|[27, 9648]|1244944|               en|The Woman in the ...|In the aftermath ...|  172.9102|/n0WS2TsNcS6dtaZK...|  2025-03-27|The Woman in the ...|false|        6.27|        37|2025-04-17 16:59:...|2025-04-23|2025-05-14|\n",
       "|false|/3lEV4CoKoeT2cZ4f...|[27, 9648]|1244944|               en|The Woman in the ...|In the aftermath ...|  157.7558|/n0WS2TsNcS6dtaZK...|  2025-03-27|The Woman in the ...|false|         6.0|        97|2025-04-27 12:53:...|2025-04-30|2025-05-21|\n",
       "|false|/3lEV4CoKoeT2cZ4f...|[27, 9648]|1244944|               en|The Woman in the ...|In the aftermath ...|  262.5169|/n0WS2TsNcS6dtaZK...|  2025-03-27|The Woman in the ...|false|         6.0|        89|2025-04-23 08:47:...|2025-04-30|2025-05-21|\n",
       "+-----+--------------------+----------+-------+-----------------+--------------------+--------------------+----------+--------------------+------------+--------------------+-----+------------+----------+--------------------+----------+----------+"
      ],
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>adult</th><th>backdrop_path</th><th>genre_ids</th><th>id</th><th>original_language</th><th>original_title</th><th>overview</th><th>popularity</th><th>poster_path</th><th>release_date</th><th>title</th><th>video</th><th>vote_average</th><th>vote_count</th><th>ingestion_time</th><th>begin_date</th><th>end_date</th></tr>\n",
       "<tr><td>false</td><td>/3lEV4CoKoeT2cZ4f...</td><td>[27, 9648]</td><td>1244944</td><td>en</td><td>The Woman in the ...</td><td>In the aftermath ...</td><td>172.9102</td><td>/n0WS2TsNcS6dtaZK...</td><td>2025-03-27</td><td>The Woman in the ...</td><td>false</td><td>6.27</td><td>37</td><td>2025-04-17 16:59:...</td><td>2025-04-23</td><td>2025-05-14</td></tr>\n",
       "<tr><td>false</td><td>/3lEV4CoKoeT2cZ4f...</td><td>[27, 9648]</td><td>1244944</td><td>en</td><td>The Woman in the ...</td><td>In the aftermath ...</td><td>157.7558</td><td>/n0WS2TsNcS6dtaZK...</td><td>2025-03-27</td><td>The Woman in the ...</td><td>false</td><td>6.0</td><td>97</td><td>2025-04-27 12:53:...</td><td>2025-04-30</td><td>2025-05-21</td></tr>\n",
       "<tr><td>false</td><td>/3lEV4CoKoeT2cZ4f...</td><td>[27, 9648]</td><td>1244944</td><td>en</td><td>The Woman in the ...</td><td>In the aftermath ...</td><td>262.5169</td><td>/n0WS2TsNcS6dtaZK...</td><td>2025-03-27</td><td>The Woman in the ...</td><td>false</td><td>6.0</td><td>89</td><td>2025-04-23 08:47:...</td><td>2025-04-30</td><td>2025-05-21</td></tr>\n",
       "</table>\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T17:13:17.957525Z",
     "start_time": "2025-04-29T17:13:17.737801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "relative_path = \"../data/letstalk_landing_zone_bdma/delta_news/entertainment\"\n",
    "absolute_path = os.path.abspath(relative_path)\n",
    "inc = IncrementalLoad(spark)\n",
    "inc.get_new_data(absolute_path, '../version')"
   ],
   "id": "91f9bfb8f572528a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CDF from version 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/29 19:13:17 WARN DeltaHistoryManager: Found Delta commit 0 with a timestamp 1745921639145 which is greater than the next commit timestamp 1745921639145.\n",
      "25/04/29 19:13:17 WARN DeltaHistoryManager: Found Delta commit 2 with a timestamp 1745921639501 which is greater than the next commit timestamp 1745921639239.\n",
      "25/04/29 19:13:17 WARN DeltaHistoryManager: Found Delta commit 3 with a timestamp 1745921639502 which is greater than the next commit timestamp 1745921639185.\n",
      "25/04/29 19:13:17 WARN DeltaHistoryManager: Found Delta commit 5 with a timestamp 1745921639603 which is greater than the next commit timestamp 1745921639436.\n",
      "25/04/29 19:13:17 WARN DeltaHistoryManager: Found Delta commit 6 with a timestamp 1745921639604 which is greater than the next commit timestamp 1745921639330.\n",
      "25/04/29 19:13:17 WARN DeltaHistoryManager: Found Delta commit 7 with a timestamp 1745921639605 which is greater than the next commit timestamp 1745921639185.\n",
      "25/04/29 19:13:17 WARN DeltaHistoryManager: Found Delta commit 8 with a timestamp 1745921639606 which is greater than the next commit timestamp 1745921639294.\n",
      "25/04/29 19:13:17 WARN DeltaHistoryManager: Found Delta commit 9 with a timestamp 1745921639607 which is greater than the next commit timestamp 1745921639089.\n",
      "25/04/29 19:13:17 WARN DeltaHistoryManager: Found Delta commit 10 with a timestamp 1745921639608 which is greater than the next commit timestamp 1745921639185.\n",
      "25/04/29 19:13:17 WARN DeltaHistoryManager: Found Delta commit 11 with a timestamp 1745921639609 which is greater than the next commit timestamp 1745921639087.\n",
      "25/04/29 19:13:17 WARN DeltaHistoryManager: Found Delta commit 0 with a timestamp 1745921639145 which is greater than the next commit timestamp 1745921639145.\n",
      "25/04/29 19:13:17 WARN DeltaHistoryManager: Found Delta commit 2 with a timestamp 1745921639501 which is greater than the next commit timestamp 1745921639239.\n",
      "25/04/29 19:13:17 WARN DeltaHistoryManager: Found Delta commit 3 with a timestamp 1745921639502 which is greater than the next commit timestamp 1745921639185.\n",
      "25/04/29 19:13:17 WARN DeltaHistoryManager: Found Delta commit 5 with a timestamp 1745921639603 which is greater than the next commit timestamp 1745921639436.\n",
      "25/04/29 19:13:17 WARN DeltaHistoryManager: Found Delta commit 6 with a timestamp 1745921639604 which is greater than the next commit timestamp 1745921639330.\n",
      "25/04/29 19:13:17 WARN DeltaHistoryManager: Found Delta commit 7 with a timestamp 1745921639605 which is greater than the next commit timestamp 1745921639185.\n",
      "25/04/29 19:13:17 WARN DeltaHistoryManager: Found Delta commit 8 with a timestamp 1745921639606 which is greater than the next commit timestamp 1745921639294.\n",
      "25/04/29 19:13:17 WARN DeltaHistoryManager: Found Delta commit 9 with a timestamp 1745921639607 which is greater than the next commit timestamp 1745921639089.\n",
      "25/04/29 19:13:17 WARN DeltaHistoryManager: Found Delta commit 10 with a timestamp 1745921639608 which is greater than the next commit timestamp 1745921639185.\n",
      "25/04/29 19:13:17 WARN DeltaHistoryManager: Found Delta commit 11 with a timestamp 1745921639609 which is greater than the next commit timestamp 1745921639087.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+---------------+--------------------+\n",
       " |              author|             content|         description|         publishedAt|              source|               title|                 url|          urlToImage|_change_type|_commit_version|   _commit_timestamp|\n",
       " +--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+---------------+--------------------+\n",
       " |          Anna Logan|Did you know Gigi...|From Hollywood to...|2025-04-26T13:17:46Z|{NULL, countryliv...|11 Celebrities Se...|https://www.count...|https://hips.hear...|      insert|             14|2025-04-29 15:15:...|\n",
       " |Anneta Konstantin...|Guy Fieri may hav...|Guy Fieri may hav...|2025-04-16T08:01:00Z|{business-insider...|Guy Fieri's kids ...|https://www.busin...|https://i.insider...|      insert|             14|2025-04-29 15:15:...|\n",
       " |        Anna Tingley|Megyn Kelly didn’...|At the Time100 ga...|2025-04-25T15:07:00Z|     {NULL, Variety}|Megyn Kelly Says ...|https://variety.c...|https://variety.c...|      insert|             14|2025-04-29 15:15:...|\n",
       " |        Anna Kaufman|Lulu Roman, a gos...|Lulu Roman, a gos...|2025-04-25T13:15:12Z|{usa-today, USA T...|Lulu Roman, gospe...|https://www.usato...|https://www.usato...|      insert|             14|2025-04-29 15:15:...|\n",
       " +--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+---------------+--------------------+,\n",
       " 14)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T17:15:01.385451Z",
     "start_time": "2025-04-29T17:15:00.919334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"source_table\", StringType(), False),\n",
    "    StructField(\"last_processed_version\", LongType(), False),\n",
    "    StructField(\"last_run_ts\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "empty_df = spark.createDataFrame([], schema)\n",
    "\n",
    "base_path = \"../data/letstalk_landing_zone_bdma\"\n",
    "control_table_path = \"control_table\"\n",
    "final_path = os.path.join(base_path, control_table_path)\n",
    "\n",
    "empty_df.write.format(\"delta\").mode(\"overwrite\").save(final_path)\n"
   ],
   "id": "ceb4211050f21d12",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "48958801fe4d1ecd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T17:13:16.784034Z",
     "start_time": "2025-04-29T17:13:16.777972Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 17,
   "source": [
    "def does_cdf_make_sense(delta_table, last_version):\n",
    "    history_df = delta_table.history()\n",
    "    for row in history_df.collect():\n",
    "        if '\"delta.enableChangeDataFeed\":\"true\"' in row['operationParameters'].get(\"properties\", \"\"):\n",
    "            cdf_start_version = row[\"version\"]\n",
    "            break\n",
    "    else:\n",
    "        cdf_start_version = None\n",
    "\n",
    "    if cdf_start_version is not None and last_version >= cdf_start_version:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_cdf_enabled(delta_table):\n",
    "    properties = delta_table.detail().selectExpr(\"properties['delta.enableChangeDataFeed']\").collect()\n",
    "    return properties[0][0] == \"true\"\n",
    "\n",
    "\n",
    "class IncrementalLoad:\n",
    "    def __init__(self, spark, base_path, table_name):\n",
    "        self.spark = spark\n",
    "        self.base_path = base_path\n",
    "        self.control_table_path = os.path.join(self.base_path, \"control_table\")\n",
    "        self.table_name = table_name\n",
    "        self.landing_path = os.path.join(self.base_path, self.table_name)\n",
    "\n",
    "\n",
    "    def update_control_table(self, latest_version):\n",
    "        new_row = spark.createDataFrame([\n",
    "            Row(source_table=self.table_name,\n",
    "                last_processed_version=latest_version,\n",
    "                last_run_ts=current_timestamp())\n",
    "        ])\n",
    "\n",
    "        control_table = DeltaTable.forPath(self.spark, self.control_table_path)\n",
    "\n",
    "        control_table.alias(\"target\").merge(\n",
    "            new_row.alias(\"source\"),\n",
    "            \"target.source_table = source.source_table\"\n",
    "        ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "\n",
    "    def enable_cdf(self):\n",
    "        self.spark.sql(f\"\"\"\n",
    "          ALTER TABLE delta.`{self.landing_path}`\n",
    "          SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "        \"\"\")\n",
    "\n",
    "\n",
    "    def get_last_processed_version(self):\n",
    "        df = self.spark.read.format(\"delta\").load(self.control_table_path)\n",
    "        result = df.filter(df.source_table == self.table_name).collect()\n",
    "        if result:\n",
    "            return result[0]['last_processed_version']\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def get_latest_version(self):\n",
    "        delta_table = DeltaTable.forPath(self.spark, self.landing_path)\n",
    "        return delta_table.history(1).select(\"version\").collect()[0][0]\n",
    "\n",
    "\n",
    "    def get_new_data(self):\n",
    "        last_processed_version = self.get_last_processed_version()\n",
    "        delta_table = DeltaTable.forPath(self.spark, self.landing_path)\n",
    "\n",
    "        cdf_enabled = is_cdf_enabled(delta_table)\n",
    "        latest_version = self.get_latest_version()\n",
    "\n",
    "        use_cdf = cdf_enabled and does_cdf_make_sense(delta_table, last_processed_version)\n",
    "\n",
    "        if use_cdf:\n",
    "            print(f\"Using CDF from version {last_processed_version}\")\n",
    "            df = self.spark.read.format(\"delta\") \\\n",
    "                .option(\"readChangeData\", \"true\") \\\n",
    "                .option(\"startingVersion\", str(last_processed_version)) \\\n",
    "                .option(\"endingVersion\", str(latest_version)) \\\n",
    "                .load(self.landing_path) \\\n",
    "                .filter(\"_change_type = 'insert'\")\n",
    "        else:\n",
    "            print(\"CDF not available — doing full load\")\n",
    "            if not cdf_enabled:\n",
    "                self.enable_cdf()\n",
    "            latest_version = self.get_latest_version()\n",
    "            df = self.spark.read.format(\"delta\") \\\n",
    "                 .option(\"versionAsOf\", latest_version) \\\n",
    "                 .load(self.landing_path)\n",
    "\n",
    "        return df, latest_version\n"
   ],
   "id": "1fa7a066a2a1cd94"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

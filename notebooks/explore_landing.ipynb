{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-29T10:25:11.108859Z",
     "start_time": "2025-04-29T10:25:11.047844Z"
    }
   },
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from delta.tables import DeltaTable\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:53:11.099062Z",
     "start_time": "2025-04-29T12:53:11.095893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_spark_session():\n",
    "    conf = (\n",
    "        pyspark.conf.SparkConf()\n",
    "        .setAppName(\"LetsTalk\")\n",
    "        .set(\n",
    "            \"spark.sql.catalog.spark_catalog\",\n",
    "            \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "        )\n",
    "        .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .set(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "        .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\")\n",
    "        .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/Users/alfio/projects/upc/BDMP2/gcs.json\")\n",
    "        .set(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "        .set(\"spark.jars\", \"../jars/gcs-connector-hadoop3-latest.jar\")\n",
    "        .setMaster(\n",
    "            \"local[*]\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    builder = pyspark.sql.SparkSession.builder.appName(\"LetsTalk\").config(conf=conf)\n",
    "    spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "    return spark\n",
    "\n"
   ],
   "id": "a0fed999dd78466a",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:53:13.628883Z",
     "start_time": "2025-04-29T12:53:13.617913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = create_spark_session()\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ],
   "id": "314d425a93c6dfe7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/29 14:53:13 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T13:16:04.569585Z",
     "start_time": "2025-04-29T13:16:04.559737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "landing_path = \"//data/letstalk_landing_zone_bdma/delta_news/entertainment\"\n",
    "deltane = DeltaTable.forPath(spark, landing_path)\n",
    "dfne = deltane.toDF()"
   ],
   "id": "16a0124018012ce5",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T13:15:33.106137Z",
     "start_time": "2025-04-29T13:15:33.101997Z"
    }
   },
   "cell_type": "code",
   "source": "new_data_df = dfne.filter(dfne.author.startswith(\"Ann\"))",
   "id": "d66c60a7e644abdd",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T13:15:35.879769Z",
     "start_time": "2025-04-29T13:15:35.334694Z"
    }
   },
   "cell_type": "code",
   "source": "new_data_df.write.format(\"delta\").mode(\"append\").save(landing_path)",
   "id": "3096743401fea141",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:55:00.539695Z",
     "start_time": "2025-04-29T12:55:00.530907Z"
    }
   },
   "cell_type": "code",
   "source": "deltane.filter(deltane.filter(''))",
   "id": "aa75e455a61814ae",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DeltaTable' object has no attribute 'insert'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[70], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdeltane\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minsert\u001B[49m()\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'DeltaTable' object has no attribute 'insert'"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T15:47:11.592401Z",
     "start_time": "2025-04-29T15:47:11.503658Z"
    }
   },
   "cell_type": "code",
   "source": "deltane.history()",
   "id": "cbc54909bce37c69",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/29 17:47:11 WARN DeltaHistoryManager: Found Delta commit 0 with a timestamp 1745921639145 which is greater than the next commit timestamp 1745921639145.\n",
      "25/04/29 17:47:11 WARN DeltaHistoryManager: Found Delta commit 2 with a timestamp 1745921639501 which is greater than the next commit timestamp 1745921639239.\n",
      "25/04/29 17:47:11 WARN DeltaHistoryManager: Found Delta commit 3 with a timestamp 1745921639502 which is greater than the next commit timestamp 1745921639185.\n",
      "25/04/29 17:47:11 WARN DeltaHistoryManager: Found Delta commit 5 with a timestamp 1745921639603 which is greater than the next commit timestamp 1745921639436.\n",
      "25/04/29 17:47:11 WARN DeltaHistoryManager: Found Delta commit 6 with a timestamp 1745921639604 which is greater than the next commit timestamp 1745921639330.\n",
      "25/04/29 17:47:11 WARN DeltaHistoryManager: Found Delta commit 7 with a timestamp 1745921639605 which is greater than the next commit timestamp 1745921639185.\n",
      "25/04/29 17:47:11 WARN DeltaHistoryManager: Found Delta commit 8 with a timestamp 1745921639606 which is greater than the next commit timestamp 1745921639294.\n",
      "25/04/29 17:47:11 WARN DeltaHistoryManager: Found Delta commit 9 with a timestamp 1745921639607 which is greater than the next commit timestamp 1745921639089.\n",
      "25/04/29 17:47:11 WARN DeltaHistoryManager: Found Delta commit 10 with a timestamp 1745921639608 which is greater than the next commit timestamp 1745921639185.\n",
      "25/04/29 17:47:11 WARN DeltaHistoryManager: Found Delta commit 11 with a timestamp 1745921639609 which is greater than the next commit timestamp 1745921639087.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "+-------+--------------------+------+--------+-----------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+--------------------+--------------------+\n",
       "|version|           timestamp|userId|userName|        operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|        userMetadata|          engineInfo|\n",
       "+-------+--------------------+------+--------+-----------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+--------------------+--------------------+\n",
       "|     14|2025-04-29 15:15:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|         13|  Serializable|        false|{numFiles -> 3, n...|                NULL|Apache-Spark/3.5....|\n",
       "|     13|2025-04-29 15:13:...|  NULL|    NULL|SET TBLPROPERTIES|{properties -> {\"...|NULL|    NULL|     NULL|         12|  Serializable|         true|                  {}|                NULL|Apache-Spark/3.5....|\n",
       "|     12|2025-04-29 12:13:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|         11|  Serializable|         true|{numFiles -> 1, n...|{\"inserted_rows\":...|Apache-Spark/3.5....|\n",
       "|     11|2025-04-29 12:13:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|         10|  Serializable|         true|{numFiles -> 1, n...|{\"inserted_rows\":...|Apache-Spark/3.5....|\n",
       "|     10|2025-04-29 12:13:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          9|  Serializable|         true|{numFiles -> 1, n...|{\"inserted_rows\":...|Apache-Spark/3.5....|\n",
       "|      9|2025-04-29 12:13:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          8|  Serializable|         true|{numFiles -> 1, n...|{\"inserted_rows\":...|Apache-Spark/3.5....|\n",
       "|      8|2025-04-29 12:13:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          7|  Serializable|         true|{numFiles -> 1, n...|{\"inserted_rows\":...|Apache-Spark/3.5....|\n",
       "|      7|2025-04-29 12:13:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          6|  Serializable|         true|{numFiles -> 1, n...|{\"inserted_rows\":...|Apache-Spark/3.5....|\n",
       "|      6|2025-04-29 12:13:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          4|  Serializable|         true|{numFiles -> 1, n...|{\"inserted_rows\":...|Apache-Spark/3.5....|\n",
       "|      5|2025-04-29 12:13:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          4|  Serializable|         true|{numFiles -> 1, n...|{\"inserted_rows\":...|Apache-Spark/3.5....|\n",
       "|      4|2025-04-29 12:13:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          3|  Serializable|         true|{numFiles -> 1, n...|{\"inserted_rows\":...|Apache-Spark/3.5....|\n",
       "|      3|2025-04-29 12:13:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          2|  Serializable|         true|{numFiles -> 1, n...|{\"inserted_rows\":...|Apache-Spark/3.5....|\n",
       "|      2|2025-04-29 12:13:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          1|  Serializable|         true|{numFiles -> 1, n...|{\"inserted_rows\":...|Apache-Spark/3.5....|\n",
       "|      1|2025-04-29 12:13:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          0|  Serializable|         true|{numFiles -> 1, n...|{\"inserted_rows\":...|Apache-Spark/3.5....|\n",
       "|      0|2025-04-29 12:13:...|  NULL|    NULL|            WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|       NULL|  Serializable|         true|{numFiles -> 1, n...|{\"inserted_rows\":...|Apache-Spark/3.5....|\n",
       "+-------+--------------------+------+--------+-----------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+--------------------+--------------------+"
      ],
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr>\n",
       "<tr><td>14</td><td>2025-04-29 15:15:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>13</td><td>Serializable</td><td>false</td><td>{numFiles -&gt; 3, n...</td><td>NULL</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>13</td><td>2025-04-29 15:13:...</td><td>NULL</td><td>NULL</td><td>SET TBLPROPERTIES</td><td>{properties -&gt; {&quot;...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>12</td><td>Serializable</td><td>true</td><td>{}</td><td>NULL</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>12</td><td>2025-04-29 12:13:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>11</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;inserted_rows&quot;:...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>11</td><td>2025-04-29 12:13:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>10</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;inserted_rows&quot;:...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>10</td><td>2025-04-29 12:13:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>9</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;inserted_rows&quot;:...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>9</td><td>2025-04-29 12:13:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>8</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;inserted_rows&quot;:...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>8</td><td>2025-04-29 12:13:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>7</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;inserted_rows&quot;:...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>7</td><td>2025-04-29 12:13:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>6</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;inserted_rows&quot;:...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>6</td><td>2025-04-29 12:13:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>4</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;inserted_rows&quot;:...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>5</td><td>2025-04-29 12:13:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>4</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;inserted_rows&quot;:...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>4</td><td>2025-04-29 12:13:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>3</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;inserted_rows&quot;:...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>3</td><td>2025-04-29 12:13:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>2</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;inserted_rows&quot;:...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>2</td><td>2025-04-29 12:13:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>1</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;inserted_rows&quot;:...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>1</td><td>2025-04-29 12:13:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>0</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;inserted_rows&quot;:...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>0</td><td>2025-04-29 12:13:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;inserted_rows&quot;:...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "</table>\n"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T13:19:01.950683Z",
     "start_time": "2025-04-29T13:19:01.936892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_data = spark.read.format(\"delta\") \\\n",
    "    .option(\"readChangeData\", \"true\") \\\n",
    "    .option(\"startingVersion\", 0) \\\n",
    "    .option(\"endingVersion\", 14) \\\n",
    "    .load(path=landing_path) \\\n",
    "    .filter(\"_change_type = 'insert'\")"
   ],
   "id": "d3fc49d93f1931f",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:53:17.306593Z",
     "start_time": "2025-04-29T12:53:17.302854Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 66,
   "source": "dfne = dfne.dropDuplicates()",
   "id": "47af411f91a66b59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:53:18.955085Z",
     "start_time": "2025-04-29T12:53:18.945071Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 67,
   "source": [
    "deltatu = DeltaTable.forPath(spark, \"../data/letstalk_landing_zone_bdma/delta_tmdb/upcoming\")\n",
    "dftu = deltatu.toDF()"
   ],
   "id": "b4beb5a6bc965c9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:49:17.234636Z",
     "start_time": "2025-04-29T12:49:17.071685Z"
    }
   },
   "cell_type": "code",
   "source": "deltatu.history()",
   "id": "d2e2e24eb17d09c5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+--------------------+--------------------+\n",
       "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|        userMetadata|          engineInfo|\n",
       "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+--------------------+--------------------+\n",
       "|      2|2025-04-29 12:14:...|  NULL|    NULL|    WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          1|  Serializable|         true|{numFiles -> 1, n...|{\"perc_rows_inser...|Apache-Spark/3.5....|\n",
       "|      1|2025-04-29 12:14:...|  NULL|    NULL|    WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          0|  Serializable|         true|{numFiles -> 1, n...|{\"perc_rows_inser...|Apache-Spark/3.5....|\n",
       "|      0|2025-04-29 12:14:...|  NULL|    NULL|    WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|       NULL|  Serializable|         true|{numFiles -> 1, n...|{\"perc_rows_inser...|Apache-Spark/3.5....|\n",
       "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+--------------------+--------------------+"
      ],
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr>\n",
       "<tr><td>2</td><td>2025-04-29 12:14:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>1</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;perc_rows_inser...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>1</td><td>2025-04-29 12:14:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>0</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;perc_rows_inser...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "<tr><td>0</td><td>2025-04-29 12:14:...</td><td>NULL</td><td>NULL</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>{&quot;perc_rows_inser...</td><td>Apache-Spark/3.5....</td></tr>\n",
       "</table>\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T11:47:55.775637Z",
     "start_time": "2025-04-29T11:47:55.672917Z"
    }
   },
   "cell_type": "code",
   "source": "dftu.groupby('id').count().orderBy('count', ascending=False).show()",
   "id": "fc2ab4db476d655f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|     id|count|\n",
      "+-------+-----+\n",
      "|1244944|    3|\n",
      "|1126166|    3|\n",
      "|1064486|    3|\n",
      "| 995926|    3|\n",
      "|1233575|    3|\n",
      "|1212855|    3|\n",
      "|1226406|    3|\n",
      "|1241436|    3|\n",
      "| 324544|    3|\n",
      "|1353117|    3|\n",
      "|1388366|    3|\n",
      "|1380415|    2|\n",
      "| 970450|    2|\n",
      "| 575265|    2|\n",
      "| 986056|    2|\n",
      "|1181107|    2|\n",
      "|1233413|    2|\n",
      "|1232546|    2|\n",
      "|1249289|    1|\n",
      "|1249213|    1|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T11:49:49.053619Z",
     "start_time": "2025-04-29T11:49:48.853633Z"
    }
   },
   "cell_type": "code",
   "source": "dftu.filter(dftu.id == 1244944)",
   "id": "8850a9322ff614c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-----+--------------------+----------+-------+-----------------+--------------------+--------------------+----------+--------------------+------------+--------------------+-----+------------+----------+--------------------+----------+----------+\n",
       "|adult|       backdrop_path| genre_ids|     id|original_language|      original_title|            overview|popularity|         poster_path|release_date|               title|video|vote_average|vote_count|      ingestion_time|begin_date|  end_date|\n",
       "+-----+--------------------+----------+-------+-----------------+--------------------+--------------------+----------+--------------------+------------+--------------------+-----+------------+----------+--------------------+----------+----------+\n",
       "|false|/3lEV4CoKoeT2cZ4f...|[27, 9648]|1244944|               en|The Woman in the ...|In the aftermath ...|  172.9102|/n0WS2TsNcS6dtaZK...|  2025-03-27|The Woman in the ...|false|        6.27|        37|2025-04-17 16:59:...|2025-04-23|2025-05-14|\n",
       "|false|/3lEV4CoKoeT2cZ4f...|[27, 9648]|1244944|               en|The Woman in the ...|In the aftermath ...|  157.7558|/n0WS2TsNcS6dtaZK...|  2025-03-27|The Woman in the ...|false|         6.0|        97|2025-04-27 12:53:...|2025-04-30|2025-05-21|\n",
       "|false|/3lEV4CoKoeT2cZ4f...|[27, 9648]|1244944|               en|The Woman in the ...|In the aftermath ...|  262.5169|/n0WS2TsNcS6dtaZK...|  2025-03-27|The Woman in the ...|false|         6.0|        89|2025-04-23 08:47:...|2025-04-30|2025-05-21|\n",
       "+-----+--------------------+----------+-------+-----------------+--------------------+--------------------+----------+--------------------+------------+--------------------+-----+------------+----------+--------------------+----------+----------+"
      ],
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>adult</th><th>backdrop_path</th><th>genre_ids</th><th>id</th><th>original_language</th><th>original_title</th><th>overview</th><th>popularity</th><th>poster_path</th><th>release_date</th><th>title</th><th>video</th><th>vote_average</th><th>vote_count</th><th>ingestion_time</th><th>begin_date</th><th>end_date</th></tr>\n",
       "<tr><td>false</td><td>/3lEV4CoKoeT2cZ4f...</td><td>[27, 9648]</td><td>1244944</td><td>en</td><td>The Woman in the ...</td><td>In the aftermath ...</td><td>172.9102</td><td>/n0WS2TsNcS6dtaZK...</td><td>2025-03-27</td><td>The Woman in the ...</td><td>false</td><td>6.27</td><td>37</td><td>2025-04-17 16:59:...</td><td>2025-04-23</td><td>2025-05-14</td></tr>\n",
       "<tr><td>false</td><td>/3lEV4CoKoeT2cZ4f...</td><td>[27, 9648]</td><td>1244944</td><td>en</td><td>The Woman in the ...</td><td>In the aftermath ...</td><td>157.7558</td><td>/n0WS2TsNcS6dtaZK...</td><td>2025-03-27</td><td>The Woman in the ...</td><td>false</td><td>6.0</td><td>97</td><td>2025-04-27 12:53:...</td><td>2025-04-30</td><td>2025-05-21</td></tr>\n",
       "<tr><td>false</td><td>/3lEV4CoKoeT2cZ4f...</td><td>[27, 9648]</td><td>1244944</td><td>en</td><td>The Woman in the ...</td><td>In the aftermath ...</td><td>262.5169</td><td>/n0WS2TsNcS6dtaZK...</td><td>2025-03-27</td><td>The Woman in the ...</td><td>false</td><td>6.0</td><td>89</td><td>2025-04-23 08:47:...</td><td>2025-04-30</td><td>2025-05-21</td></tr>\n",
       "</table>\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def does_cdf_make_sense(delta_table, last_version):\n",
    "    history_df = delta_table.history()\n",
    "    for row in history_df.collect():\n",
    "        print(row['operationParameters'])\n",
    "        if '\"delta.enableChangeDataFeed\":\"true\"' in row['operationParameters'].get(\"properties\", \"\"):\n",
    "            cdf_start_version = row[\"version\"]\n",
    "            break\n",
    "    else:\n",
    "        cdf_start_version = None\n",
    "\n",
    "    if cdf_start_version is not None and last_version >= cdf_start_version:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_cdf_enabled(delta_table):\n",
    "    properties = delta_table.detail().selectExpr(\"properties['delta.enableChangeDataFeed']\").collect()\n",
    "    return properties[0][0] == \"true\"\n",
    "\n",
    "\n",
    "def get_last_version(version_tracking_file):\n",
    "    if os.path.exists(version_tracking_file):\n",
    "        with open(version_tracking_file, \"r\") as f:\n",
    "            last_version = int(f.read().strip())\n",
    "    else:\n",
    "        last_version = 0\n",
    "\n",
    "    return last_version\n",
    "\n",
    "\n",
    "class IncrementalLoad:\n",
    "    def __init__(self, spark):\n",
    "        self.spark = spark\n",
    "\n",
    "    def enable_cdf(self, landing_path):\n",
    "        self.spark.sql(f\"\"\"\n",
    "          ALTER TABLE delta.`{landing_path}`\n",
    "          SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "        \"\"\")\n",
    "\n",
    "\n",
    "    def get_latest_version(self, landing_path):\n",
    "        delta_table = DeltaTable.forPath(self.spark, landing_path)\n",
    "        return delta_table.history(1).select(\"version\").collect()[0][0]\n",
    "\n",
    "\n",
    "    def get_new_data(self, landing_path, version_tracking_file):\n",
    "        last_version = get_last_version(version_tracking_file)\n",
    "        delta_table = DeltaTable.forPath(self.spark, landing_path)\n",
    "\n",
    "        cdf_enabled = is_cdf_enabled(delta_table)\n",
    "        latest_version = self.get_latest_version(landing_path)\n",
    "\n",
    "        use_cdf = cdf_enabled and does_cdf_make_sense(delta_table, last_version)\n",
    "\n",
    "        if use_cdf:\n",
    "            print(f\"Using CDF from version {last_version}\")\n",
    "            df = self.spark.read.format(\"delta\") \\\n",
    "                .option(\"readChangeData\", \"true\") \\\n",
    "                .option(\"startingVersion\", str(last_version)) \\\n",
    "                .option(\"endingVersion\", str(latest_version)) \\\n",
    "                .load(landing_path) \\\n",
    "                .filter(\"_change_type = 'insert'\")\n",
    "        else:\n",
    "            print(\"CDF not available â€” doing full load\")\n",
    "            if not cdf_enabled:\n",
    "                self.enable_cdf(landing_path)\n",
    "            latest_version = self.get_latest_version(landing_path)\n",
    "            df = self.spark.read.format(\"delta\") \\\n",
    "                 .option(\"versionAsOf\", latest_version) \\\n",
    "                 .load(landing_path)\n",
    "\n",
    "        return df, latest_version\n"
   ],
   "id": "1fa7a066a2a1cd94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "relative_path = \"../data/letstalk_landing_zone_bdma/delta_news/entertainment\"\n",
    "absolute_path = os.path.abspath(relative_path)\n",
    "inc = IncrementalLoad(spark)\n",
    "inc.get_new_data(absolute_path, '../version')"
   ],
   "id": "91f9bfb8f572528a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "relative_path = \"../data/letstalk_landing_zone_bdma/delta_news/entertainment\"\n",
    "absolute_path = os.path.abspath(relative_path)\n",
    "get_new_data(absolute_path, '../version')"
   ],
   "id": "d00cc89e12e26c49"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
    "\n",
    "# Define schema for control table\n",
    "schema = StructType([\n",
    "    StructField(\"source_table\", StringType(), False),\n",
    "    StructField(\"last_processed_version\", LongType(), False),\n",
    "    StructField(\"last_run_ts\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Create an empty DataFrame\n",
    "empty_df = spark.createDataFrame([], schema)\n",
    "\n",
    "# Save it as a Delta table (local or cloud path)\n",
    "control_table_path = \"control_table\"  # or \"gs://bucket/control_table\"\n",
    "empty_df.write.format(\"delta\").mode(\"overwrite\").save(control_table_path)\n"
   ],
   "id": "ceb4211050f21d12"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
